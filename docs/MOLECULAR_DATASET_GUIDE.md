# ğŸ§¬ Molecular Dataset Generation & Fine-Tuning Guide

**Complete guide for downloading real drug molecules and preparing them for AI model fine-tuning.**

## ğŸ¯ What You Get

This system provides **production-ready molecular datasets** by:

âœ… **Downloading real molecules** from ChEMBL/ZINC (60,000+ compounds)  
âœ… **Converting SMILES to 2D images** locally using RDKit (NOT web scraping!)  
âœ… **Generating 3D conformers** for docking and advanced GNNs  
âœ… **Preparing datasets** for Vision, Multimodal, and Graph models  
âœ… **RTX 4060 optimization** with proper batch sizes and memory management  

## ğŸš€ Quick Start (One Command)

```bash
# 1. Navigate to your pipeline
cd Multi-Agent-Drug-Discovery

# 2. Install dependencies (first time only)
pip install -r requirements.txt

# 3. Build complete molecular dataset (60k molecules)
python examples/build_molecular_dataset.py
```

**Expected Output:**
```
ğŸ§¬ MOLECULAR DATASET BUILDER FOR AI DRUG DISCOVERY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   
   âœ… Downloads REAL molecules from ChEMBL (not random images!)
   âœ… Converts SMILES â†’ 2D/3D representations locally 
   âœ… Optimized for RTX 4060 + i7 14th gen
   âœ… Ready for Vision/Multimodal/Graph model fine-tuning
   âœ… Production-grade chemical intelligence

ğŸ“Š Dataset Configuration
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š Number of molecules to download (default: 60000): 
ğŸ”— Data source options:
  1. ChEMBL (recommended) - Curated bioactive molecules
  2. ZINC - Purchasable drug-like compounds
Choose source (1/2, default: 1): 
ğŸ“ Output directory (default: drug_dataset): 
ğŸ§¬ Generate 3D molecular structures?
Generate 3D (y/n, default: y): y

ğŸ“Š Estimated Requirements:
  â±ï¸  Time: ~45.2 minutes
  ğŸ’¾ Disk: ~2.8 GB
  ğŸ§¬ Molecules: 60,000
  ğŸ–¼ï¸  2D Images: 60,000
  ğŸ”¬ 3D Structures: 60,000

ğŸš¦ Ready to proceed?
Start building dataset? (y/n): y

ğŸ—ï¸ Building Molecular Dataset
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸŒ Downloading from ChEMBL...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [12:34<00:00, 79.6it/s]
âœ… Downloaded 60,000 valid molecules from ChEMBL

ğŸ§® Calculating molecular properties...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [03:45<00:00, 266.2it/s]
ğŸ“Š Saved metadata to drug_dataset/metadata/molecules_metadata.csv

ğŸ–¼ï¸ Generating 2D images for 60,000 molecules...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [18:22<00:00, 54.4it/s]
âœ… Generated 60,000 2D images

ğŸ§¬ Generating 3D conformers for 60,000 molecules...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [25:15<00:00, 39.6it/s]
âœ… Generated 58,943 3D conformers

ğŸ”„ Created splits: 48,000 train, 12,000 val

ğŸ”¥ Preparing Fine-Tuning Data
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ–¼ï¸ Preparing vision datasets...
âœ… Vision datasets ready: 48,000 train, 12,000 val
ğŸ”— Preparing multimodal datasets...
âœ… Multimodal datasets ready: 48,000 train, 12,000 val
ğŸ§¬ Preparing graph datasets...
âœ… Graph datasets ready: 48,000 train, 12,000 val

âœ… Fine-tuning datasets ready!
  ğŸ–¼ï¸  Vision models: âœ…
  ğŸ”— Multimodal models: âœ…
  ğŸ§¬ Graph models: âœ…
  ğŸ® Device: cuda

ğŸ‰ Dataset Build Complete!
  â±ï¸  Time: 42.3 minutes
  ğŸ§¬ Molecules: 60,000
  ğŸ–¼ï¸  Images: 60,000
  ğŸ”¬ 3D Structures: 58,943
```

## ğŸ“ What Gets Created

Your dataset will have this structure:

```
drug_dataset/
â”œâ”€â”€ smiles/
â”‚   â””â”€â”€ molecules.smi                     # 60k SMILES strings
â”œâ”€â”€ images_2d/
â”‚   â”œâ”€â”€ mol_000001.png                   # 2D molecular images  
â”‚   â”œâ”€â”€ mol_000002.png                   # (300x300 PNG files)
â”‚   â””â”€â”€ ... (60,000 files)
â”œâ”€â”€ sdf_3d/
â”‚   â”œâ”€â”€ mol_000001.sdf                   # 3D conformers
â”‚   â”œâ”€â”€ mol_000002.sdf                   # (SDF format)
â”‚   â””â”€â”€ ... (~59,000 files)
â”œâ”€â”€ metadata/
â”‚   â””â”€â”€ molecules_metadata.csv           # Molecular properties
â”œâ”€â”€ splits/
â”‚   â””â”€â”€ train_val_splits.json           # 80/20 train/val split
â”œâ”€â”€ fine_tuning_ready/                   # PyTorch datasets
â”‚   â”œâ”€â”€ vision/                          # For ResNet, ViT, etc.
â”‚   â”œâ”€â”€ multimodal/                      # For CLIP, BLIP
â”‚   â”œâ”€â”€ graph/                           # For GNNs
â”‚   â””â”€â”€ configs/
â””â”€â”€ dataset_summary.json                 # Complete statistics
```

## ğŸ”¥ Fine-Tuning Examples

### 1ï¸âƒ£ Vision Models (ResNet, ViT, EfficientNet)

Fine-tune popular vision models on molecular images:

```bash
# Fine-tune ResNet50 to predict LogP (lipophilicity)
python examples/fine_tune_vision.py --model resnet50 --property logp --epochs 20

# Fine-tune Vision Transformer for molecular weight
python examples/fine_tune_vision.py --model vit_b_16 --property molecular_weight --epochs 15

# Fine-tune EfficientNet for hydrogen bond acceptors
python examples/fine_tune_vision.py --model efficientnet_b0 --property hba --epochs 25
```

**Expected Output:**
```
ğŸ”¥ Molecular Vision Fine-Tuning
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¤– Model: resnet50
ğŸ“Š Property: logp
â±ï¸  Epochs: 20
ğŸ“¦ Batch size: 32
ğŸ¯ Learning rate: 0.0001
ğŸ“ Dataset: drug_dataset
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ® Using device: cuda
   GPU: NVIDIA GeForce RTX 4060
   Memory: 8.0 GB

ğŸ“Š Loading datasets...
âœ… Datasets loaded:
   Train: 48,000 samples
   Val: 12,000 samples

ğŸ¤– Creating resnet50 model...
âœ… Model created with 23,512,193 parameters

ğŸ”¥ Starting training for 20 epochs...
ğŸ® Device: cuda
ğŸ“Š Properties: ['logp']

ğŸ“ˆ Epoch 1/20
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [02:45<00:00, 9.08it/s, Loss=1.2543]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:32<00:00, 11.67it/s]
  ğŸ”¸ Train Loss: 1.2543
  ğŸ”¸ Val Loss: 0.9876
  ğŸ”¸ Val RÂ²: 0.6234
  âœ… New best model saved! Val Loss: 0.9876

ğŸ“ˆ Epoch 2/20
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [02:43<00:00, 9.18it/s, Loss=0.8932]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:31<00:00, 11.89it/s]
  ğŸ”¸ Train Loss: 0.8932
  ğŸ”¸ Val Loss: 0.7651
  ğŸ”¸ Val RÂ²: 0.7189
  âœ… New best model saved! Val Loss: 0.7651

... (continues for 20 epochs)

ğŸ‰ Training Complete!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š Best validation loss: 0.4523
â±ï¸  Total time: 52.7 minutes
ğŸ“ˆ Epochs completed: 20

ğŸš€ To use your trained model:
   model = MolecularPropertyPredictor('resnet50', 1)
   checkpoint = torch.load('models/resnet50_logp/best_model.pth')
   model.load_state_dict(checkpoint['model_state_dict'])
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 2ï¸âƒ£ Using Your Trained Model

```python
import torch
from examples.fine_tune_vision import MolecularPropertyPredictor

# Load your trained model
model = MolecularPropertyPredictor('resnet50', 1)
checkpoint = torch.load('models/resnet50_logp/best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Predict LogP for new molecular images
from PIL import Image
import torchvision.transforms as transforms

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load and predict
image = Image.open('drug_dataset/images_2d/mol_000001.png').convert('RGB')
image_tensor = transform(image).unsqueeze(0)

with torch.no_grad():
    logp_prediction = model(image_tensor)
    print(f"Predicted LogP: {logp_prediction.item():.2f}")
```

## ğŸ® RTX 4060 Optimization Tips

Your setup is **perfect** for molecular AI! Here's how to maximize performance:

### âœ… Optimal Settings (Already Applied)
```python
# Batch sizes optimized for 8GB VRAM
VISION_BATCH_SIZE = 32        # Perfect for ResNet/ViT
GRAPH_BATCH_SIZE = 64         # GNNs are more memory efficient
MULTIMODAL_BATCH_SIZE = 16    # CLIP uses more memory

# CPU settings for i7 14th gen
NUM_WORKERS = 4               # Good for your CPU
GRADIENT_ACCUMULATION = 2     # Effective batch size = 64
```

### ğŸš€ Speed Optimizations
```python
# Mixed precision training (automatic 2x speedup)
from torch.cuda.amp import autocast, GradScaler

with autocast():
    predictions = model(images)
    loss = criterion(predictions, targets)

# Memory optimizations
torch.backends.cudnn.benchmark = True  # Optimize for consistent input sizes
pin_memory = True                       # Faster CPUâ†’GPU transfer
```

### ğŸ“Š Memory Usage
```
RTX 4060 (8GB VRAM) Usage:
â”œâ”€â”€ ResNet50 + batch_32: ~6.2GB âœ…
â”œâ”€â”€ EfficientNet + batch_32: ~5.8GB âœ…  
â”œâ”€â”€ ViT-B/16 + batch_24: ~7.1GB âœ…
â””â”€â”€ Available: ~1-2GB for system
```

## ğŸ§¬ Dataset Customization

### Custom Property Targets

```python
from src.data.fine_tuning_prep import FineTuningConfig

# Train on multiple properties simultaneously
config = FineTuningConfig(
    property_targets=[
        "molecular_weight",
        "logp", 
        "hba",
        "hbd",
        "tpsa",
        "aromatic_rings"
    ]
)

# Or focus on specific drug-like properties
config = FineTuningConfig(
    property_targets=["logp", "tpsa"],  # Lipophilicity + Polar surface area
    filter_drug_like=True              # Apply Lipinski's Rule of Five
)
```

### Different Data Sources

```python
from src.data.dataset_builder import DatasetConfig

# Use ZINC instead of ChEMBL (purchasable compounds)
config = DatasetConfig(
    source="zinc",
    total_molecules=100000,
    filter_drug_like=True
)

# Smaller, faster dataset for testing
config = DatasetConfig(
    source="chembl",
    total_molecules=5000,
    generate_3d_conformers=False  # Skip 3D for speed
)
```

## ğŸ§  What You Can Train

### ğŸ–¼ï¸ Vision Models
- **ResNet50/101**: Proven architecture, fast training
- **EfficientNet**: Best accuracy-to-speed ratio  
- **Vision Transformer (ViT)**: State-of-the-art, attention-based
- **Custom CNNs**: For specialized molecular features

### ğŸ”— Multimodal Models  
- **CLIP**: Learn joint image-text representations
- **BLIP**: Image captioning for molecules
- **Custom**: Combine molecular images + property descriptions

### ğŸ§¬ Graph Neural Networks
- **GCN**: Graph Convolutional Networks
- **GAT**: Graph Attention Networks
- **GraphSAINT**: Scalable sampling-based GNNs
- **Custom**: Molecular-specific architectures

## ğŸ”¬ Advanced Usage

### Multi-Property Prediction

```python
# Train one model to predict multiple properties
model = MolecularPropertyPredictor(
    model_name="resnet50",
    num_properties=5,  # MW, LogP, HBA, HBD, TPSA
    pretrained=True
)

# Loss will be MSE across all properties
criterion = nn.MSELoss()
```

### Custom Augmentations

```python
# Heavy augmentation for small datasets
config = FineTuningConfig(
    augmentation_strength="strong",  # Rotation, flip, color jitter
    image_size=256,                  # Higher resolution
    batch_size=24                    # Reduce batch for larger images
)
```

### Transfer Learning

```python
# Start with ImageNet â†’ fine-tune on molecular images
model = MolecularPropertyPredictor(
    model_name="resnet50",
    pretrained=True,    # Use ImageNet weights
    dropout=0.5        # Higher dropout for transfer learning
)

# Freeze backbone, train only prediction head
for param in model.backbone.parameters():
    param.requires_grad = False
```

## ğŸ“Š Expected Performance

Based on molecular property prediction benchmarks:

| Property | Expected RÂ² | Training Time (RTX 4060) | Best Model |
|----------|-------------|--------------------------|------------|
| **LogP** | 0.75-0.85 | 45-60 min | EfficientNet-B3 |
| **Molecular Weight** | 0.80-0.90 | 40-55 min | ResNet50 |
| **TPSA** | 0.70-0.80 | 50-65 min | ViT-B/16 |
| **Solubility** | 0.65-0.75 | 55-70 min | ResNet101 |

## ğŸ› ï¸ Troubleshooting

### Common Issues

**Out of Memory Error:**
```bash
# Reduce batch size
python examples/fine_tune_vision.py --model resnet50 --batch_size 16

# Or use gradient accumulation
# (Effective batch size = 16 * 4 = 64)
```

**Slow Data Loading:**
```python
# Reduce num_workers if CPU bottleneck
config = FineTuningConfig(num_workers=2)
```

**Poor Performance:**
```python
# Try different models/settings
python examples/fine_tune_vision.py --model efficientnet_b0 --lr 2e-4 --epochs 30
```

### Performance Monitoring

```bash
# Monitor GPU usage
watch -n 0.5 nvidia-smi

# Monitor training progress  
tensorboard --logdir=logs/
```

## ğŸ¯ Next Steps

1. **Start with basic vision training** using the example above
2. **Experiment with different models** (ResNet â†’ EfficientNet â†’ ViT)
3. **Try multi-property prediction** for comprehensive models
4. **Explore multimodal approaches** combining images + text
5. **Scale up** with larger datasets (100k+ molecules)

## ğŸš€ Production Deployment

Once trained, deploy your models:

```python
# Save for deployment
torch.jit.save(torch.jit.script(model), "molecular_predictor.pt")

# Load in production
model = torch.jit.load("molecular_predictor.pt")
model.eval()
```

---

**ğŸ‰ You now have a complete molecular dataset generation and fine-tuning pipeline optimized for your RTX 4060 + i7 14th gen setup!**

Ready to discover drugs with AI? ğŸ§¬